{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics.pairwise import laplacian_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import dionysus as d\n",
    "import sklearn_tda as tda\n",
    "\n",
    "from biomarker.data_collection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_KEYS = [206, 205, 184, 183, 82, 81, 45]#, 85, 135, 192]\n",
    "TEST_KEYS = [217, 216]#, 215, 214, 213, 212, 211, 210, 209]\n",
    "NUM_TEST = 10\n",
    "LIM = 5\n",
    "MUL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diagram(points, k=3):\n",
    "    r = np.max(pdist(points, 'euclidean'))\n",
    "    f = d.fill_rips(points, k, r)\n",
    "    m = d.homology_persistence(f)\n",
    "    return d.init_diagrams(m, f)\n",
    "\n",
    "def get_max_diag_dim(dgms, h=1):\n",
    "    m = 0\n",
    "    for d in dgms:\n",
    "        n = len(d[h])\n",
    "        if n > m:\n",
    "            m = n\n",
    "    return m\n",
    "\n",
    "\n",
    "def compute_sample_weights(y, lim=LIM, mul=MUL, up_lim=None, up_mul=None):\n",
    "    ret = np.ones(y.shape[0])\n",
    "    for i in range(y.shape[0]):\n",
    "        v = y[i]\n",
    "        if v < lim:\n",
    "            ret[i] = np.abs(lim-v)*mul\n",
    "        if up_lim is not None:\n",
    "            if v > up_lim:\n",
    "                if up_mul is not None:\n",
    "                    ret[i] = np.abs(lim-v)*up_mul\n",
    "                else:\n",
    "                    ret[i] = np.abs(lim-v)*mul\n",
    "    return ret\n",
    "\n",
    "def create_train_test_matrices(train_keys, test_keys):\n",
    "    excel = parse_master_file(exclude_keys=EXCLUDE_KEYS)\n",
    "    test_idxs = list(excel[np.isin(excel['Key'], test_keys)].index)\n",
    "    excel_test = excel.iloc[test_idxs]\n",
    "    L_test = get_filename_list(excel_test['Associated data'])\n",
    "    y_test = excel_test['Output: logK'].values\n",
    "    y_buck_test = excel_test['Output: logKbucket'].values\n",
    "    \n",
    "    excel_train = excel.drop(test_idxs, axis=0)\n",
    "    L_train = get_filename_list(excel_train['Associated data'])\n",
    "    y_train = excel_train['Output: logK'].values\n",
    "    y_buck_train = excel_train['Output: logKbucket'].values\n",
    "    \n",
    "    x4_train, x4_dims = create_x4_matrix(L_train, return_dims=True)\n",
    "    x4_train, x4_dims = create_x4_matrix(L_train, return_dims=True)\n",
    "    x5_train, x5_dims = create_x5_matrix(L_train, return_dims=True)\n",
    "    x6_train, x6_dims = create_x6_matrix(L_train, return_dims=True)\n",
    "    x7_train, x7_dims = create_x7_matrix(L_train, return_dims=True)\n",
    "    \n",
    "    x4_test = create_x4_matrix(L_test, max_dims=x4_dims)\n",
    "    x4_test = create_x4_matrix(L_test, max_dims=x4_dims)\n",
    "    x5_test = create_x5_matrix(L_test, max_dims=x5_dims)\n",
    "    x6_test = create_x6_matrix(L_test, max_dims=x6_dims)\n",
    "    x7_test = create_x7_matrix(L_test, max_dims=x7_dims)\n",
    "    \n",
    "    x40_x47_train = excel_train.iloc[:, 3:-2]\n",
    "    master_train, master_names = prepare_master(x40_x47_train)\n",
    "    \n",
    "    x40_x47_test = excel_test.iloc[:, 3:-2]\n",
    "    master_test, master_names = prepare_master(x40_x47_test)\n",
    "    \n",
    "    X_train = np.hstack((x4_train,x4_train,x5_train,x6_train,x7_train))\n",
    "    X_test = np.hstack((x4_test, x4_test, x5_test, x6_test, x7_test))\n",
    "    \n",
    "    X = np.vstack((X_train,X_test))\n",
    "    K = rbf_kernel(X)\n",
    "    X_train = K[:len(y_train), :len(y_train)]\n",
    "    X_test = K[len(y_train):, :len(y_train)]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def create_train_test_diagram_kernels_x4(train_keys, test_keys, h=1):\n",
    "    excel = parse_master_file(exclude_keys=EXCLUDE_KEYS)\n",
    "    test_idxs = list(excel[np.isin(excel['Key'], test_keys)].index)\n",
    "    excel_test = excel.iloc[test_idxs]\n",
    "    L_test = get_filename_list(excel_test['Associated data'])\n",
    "    y_test = excel_test['Output: logK'].values\n",
    "    y_buck_test = excel_test['Output: logKbucket'].values\n",
    "    \n",
    "    excel_train = excel.drop(test_idxs, axis=0)\n",
    "    L_train = get_filename_list(excel_train['Associated data'])\n",
    "    y_train = excel_train['Output: logK'].values\n",
    "    y_buck_train = excel_train['Output: logKbucket'].values\n",
    "    \n",
    "    print('Computing Training Diagrams')\n",
    "    diags_train = []\n",
    "    for l in L_train:\n",
    "        diags_train.append(compute_diagram(parse_x4(l)[['Val1', 'Val2', 'Val3']].values))\n",
    "    for i in range(len(diags_train)):\n",
    "        if len(diags_train[i][h]) == 0:\n",
    "            diags_train[i] = np.array([[0,0]])\n",
    "        else:\n",
    "            diags_train[i] = np.array([[pt.birth,pt.death] for pt in diags_train[i][h]])\n",
    "    \n",
    "    print('Computing Test Diagrams')\n",
    "    diags_test = []\n",
    "    for l in L_test:\n",
    "        diags_test.append(compute_diagram(parse_x4(l)[['Val1', 'Val2', 'Val3']].values))\n",
    "    for i in range(len(diags_test)):\n",
    "        if len(diags_test[i][h]) == 0:\n",
    "            diags_test[i] = np.array([[0,0]])\n",
    "        else:\n",
    "            diags_test[i] = np.array([[pt.birth,pt.death] for pt in diags_test[i][h]])\n",
    "    \n",
    "    print('Computing Kernel')\n",
    "    \n",
    "    dd = diags_train + diags_test\n",
    "    SW = tda.SlicedWassersteinKernel(num_directions=1, bandwidth=1.)\n",
    "    \n",
    "    D = SW.fit_transform(dd)\n",
    "    \n",
    "    D_train = D[:len(y_train), :len(y_train)]\n",
    "    D_test = D[len(y_train):, :len(y_train)]\n",
    "    return (D_train, y_train), (D_test, y_test)\n",
    "      \n",
    "    \n",
    "def create_test_diagram_kernels(h=1, num_directions=1, bandwidth=1.0, x_type='x1', exclude_keys=EXCLUDE_KEYS):\n",
    "    excel = parse_master_file(exclude_keys=exclude_keys)\n",
    "    excel_test = excel\n",
    "    L_test = get_filename_list(excel_test['Associated data'])\n",
    "    y_test = excel_test['Output: logK'].values\n",
    "    y_buck_test = excel_test['Output: logKbucket'].values\n",
    "    \n",
    "    print('Computing Test Diagrams')\n",
    "    diags_test = []\n",
    "    if x_type == 'x1':\n",
    "        for l in L_test:\n",
    "                diags_test.append(compute_diagram(parse_x1(l)[['X', 'Y', 'Z']].values))\n",
    "    if x_type == 'x4':\n",
    "        for l in L_test:\n",
    "            diags_test.append(compute_diagram(parse_x4(l)[['Val1', 'Val2', 'Val3']].values))\n",
    "    if x_type == 'x5':\n",
    "        for l in L_test:\n",
    "            diags_test.append(compute_diagram(parse_x5(l)[['X', 'Y', 'Z']].values))\n",
    "    for i in range(len(diags_test)):\n",
    "        if len(diags_test[i][h]) == 0:\n",
    "            diags_test[i] = np.array([[0,0]])\n",
    "        else:\n",
    "            diags_test[i] = np.array([[pt.birth,pt.death] for pt in diags_test[i][h]])\n",
    "    \n",
    "    print('Computing Kernel')\n",
    "    \n",
    "    dd = diags_test\n",
    "    SW = tda.SlicedWassersteinKernel(num_directions=num_directions, bandwidth=bandwidth)\n",
    "    \n",
    "    D = SW.fit_transform(dd)\n",
    "    \n",
    "    return (D, y_test)\n",
    "\n",
    "def get_diag(k, h=1): \n",
    "    excel = parse_master_file(exclude_keys=EXCLUDE_KEYS)\n",
    "    l = get_filename_list(excel[excel['Key'] == k]['Associated data'])[0]\n",
    "    dgm = compute_diagram(parse_x4(l)[['Val1', 'Val2', 'Val3']].values)\n",
    "    return dgm\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excel = parse_master_file(exclude_keys=EXCLUDE_KEYS).reset_index(drop=True)\n",
    "keys = list(excel['Key'])\n",
    "L = get_filename_list(excel['Associated data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_keys_min_test = [k for k in EXCLUDE_KEYS if k not in TEST_KEYS]\n",
    "# excel_test = parse_master_file(exclude_keys=(keys+exclude_keys_min_test)).reset_index(drop=True)\n",
    "# L_test = get_filename_list(excel_test['Associated data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x10_x17 = excel.iloc[:, 3:-3]\n",
    "x10_x17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master, master_names = prepare_master(x10_x17)\n",
    "K_train = master\n",
    "K_train = rbf_kernel(K_train)\n",
    "K_test = np.delete(K_train[191:193], [191,192], axis=1)\n",
    "K_train = np.delete(np.delete(K_train, [191,192], axis=0), [191,192], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train_x3 = create_x3_matrix(L)\n",
    "D_train_x3 = laplacian_kernel(D_train_x3)\n",
    "D_test_x3 = np.delete(D_train_x3[191:193], [191,192], axis=1)\n",
    "D_train_x3 = np.delete(np.delete(D_train_x3, [191,192], axis=0), [191,192], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train_x6 = create_x6_matrix(L, max_dims=(50, 6))\n",
    "D_train_x6 = laplacian_kernel(D_train_x6)\n",
    "D_test_x6 = np.delete(D_train_x6[191:193], [191,192], axis=1)\n",
    "D_train_x6 = np.delete(np.delete(D_train_x6, [191,192], axis=0), [191,192], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train_x5b = create_x5_matrix(L)[:,0].reshape(-1, 1)\n",
    "D_train_x5b = rbf_kernel(D_train_x5b)\n",
    "D_test_x5b = np.delete(D_train_x5b[191:193], [191,192], axis=1)\n",
    "D_train_x5b = np.delete(np.delete(D_train_x5b, [191,192], axis=0), [191,192], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train_x7 = create_x7_matrix(L)\n",
    "D_train_x7 = rbf_kernel(D_train_x7)\n",
    "D_test_x7 = np.delete(D_train_x7[191:193], [191,192], axis=1)\n",
    "D_train_x7 = np.delete(np.delete(D_train_x7, [191,192], axis=0), [191,192], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_x1 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x1')\n",
    "# test_set_x4 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x4')\n",
    "# test_set_x5 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x5')\n",
    "\n",
    "# D_x1 = test_set_x1[0]\n",
    "# D_x4 = test_set_x4[0]\n",
    "# D_x5 = test_set_x5[0]\n",
    "# y = test_set_x1[1]\n",
    "\n",
    "def remove_for_loo(D, i, kernel=True):\n",
    "    if kernel:\n",
    "        D_train = np.delete(np.delete(D, i, axis=0), i, axis=1)\n",
    "        D_test = np.delete(D[i], i, axis=0).reshape(1,-1)\n",
    "    else:\n",
    "        D_train = np.delete(D,i,axis=0)\n",
    "        D_test = D[i].reshape(1,-1)\n",
    "    return D_train, D_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Gammas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "Cs = [0.1, 1, 5, 10]\n",
    "Epsilons = [1e-4, 1e-2, 1e-1, 1, 10]\n",
    "num_directions = [1, 2]\n",
    "bandwidths = [1.0, 2.0]\n",
    "lims = [1, 2, 5]\n",
    "muls = [0.1, 1, 2, 5, 10]\n",
    "\n",
    "best_params = [0,0,0,0,0,0,0]\n",
    "best_operators = None\n",
    "best_score = float('inf')\n",
    "best_cvs = None\n",
    "operator_seqs = list(itertools.product([operator.add,operator.mul], repeat=6))\n",
    "\n",
    "up_lim = 0\n",
    "up_mul = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "for direction in num_directions:\n",
    "    for bandwidth in bandwidths:\n",
    "        train_set_x1 = create_test_diagram_kernels(num_directions=direction, bandwidth=bandwidth, h=1, x_type='x1')\n",
    "        train_set_x4 = create_test_diagram_kernels(num_directions=direction, bandwidth=bandwidth, h=1, x_type='x4')\n",
    "        train_set_x5 = create_test_diagram_kernels(num_directions=direction, bandwidth=bandwidth, h=1, x_type='x5')\n",
    "        D_train_x1 = train_set_x1[0]\n",
    "        D_train_x4 = train_set_x4[0]\n",
    "        D_train_x5 = train_set_x5[0]\n",
    "        \n",
    "        D_test_x1 = np.delete(D_train_x1[191:193], [191,192], axis=1)\n",
    "        D_test_x4 = np.delete(D_train_x4[191:193], [191,192], axis=1)\n",
    "        D_test_x5 = np.delete(D_train_x5[191:193], [191,192], axis=1)\n",
    "        \n",
    "        D_train_x1 = np.delete(np.delete(D_train_x1, [191,192], axis=0), [191,192], axis=1)\n",
    "        D_train_x4 = np.delete(np.delete(D_train_x4, [191,192], axis=0), [191,192], axis=1)\n",
    "        D_train_x5 = np.delete(np.delete(D_train_x5, [191,192], axis=0), [191,192], axis=1)\n",
    "        \n",
    "        y_train = train_set_x1[1]\n",
    "        y_test = y_train[191:193]\n",
    "        y_train = np.delete(y_train, [191,192], axis=0)\n",
    "        \n",
    "        \n",
    "        for lim in lims:\n",
    "            for mul in muls:\n",
    "                for gamma in Gammas:\n",
    "#                     master, master_names = prepare_master(x10_x17)\n",
    "                    K_train = master\n",
    "                    K_train = rbf_kernel(K_train, gamma=gamma)\n",
    "                    K_test = np.delete(K_train[191:193], [191,192], axis=1)\n",
    "                    K_train = np.delete(np.delete(K_train, [191,192], axis=0), [191,192], axis=1)\n",
    "                    \n",
    "                    D_train_x3 = create_x3_matrix(L)\n",
    "                    D_train_x3 = laplacian_kernel(D_train_x3, gamma=gamma)\n",
    "                    D_test_x3 = np.delete(D_train_x3[191:193], [191,192], axis=1)\n",
    "                    D_train_x3 = np.delete(np.delete(D_train_x3, [191,192], axis=0), [191,192], axis=1)\n",
    "                    \n",
    "#                     D_train_x5b = create_x5_matrix(L)[:,0].reshape(-1,1)\n",
    "#                     D_train_x5b = rbf_kernel(D_train_x5b, gamma=gamma)\n",
    "#                     D_test_x5b = np.delete(D_train_x5b[191:193], [191,192], axis=1)\n",
    "#                     D_train_x5b = np.delete(np.delete(D_train_x5b, [191,192], axis=0), [191,192], axis=1)\n",
    "                    \n",
    "                    D_train_x6 = create_x6_matrix(L, max_dims=(50, 6))\n",
    "                    D_train_x6 = laplacian_kernel(D_train_x6, gamma=gamma)\n",
    "                    D_test_x6 = np.delete(D_train_x6[191:193], [191,192], axis=1)\n",
    "                    D_train_x6 = np.delete(np.delete(D_train_x6, [191,192], axis=0), [191,192], axis=1)\n",
    "                    \n",
    "                    D_train_x7 = create_x7_matrix(L)\n",
    "                    D_train_x7 = rbf_kernel(D_train_x7, gamma=gamma)\n",
    "                    D_test_x7 = np.delete(D_train_x7[191:193], [191,192], axis=1)\n",
    "                    D_train_x7 = np.delete(np.delete(D_train_x7, [191,192], axis=0), [191,192], axis=1)      \n",
    "                    \n",
    "                    for epsilon in Epsilons:\n",
    "                        for c in Cs:\n",
    "                            for operator_seq in operator_seqs:\n",
    "                                \n",
    "                                DD_train = operator_seq[5](operator_seq[4](operator_seq[3](operator_seq[2](operator_seq[1](operator_seq[0](D_train_x1, D_train_x3),D_train_x4), D_train_x5), D_train_x6), D_train_x7), K_train)\n",
    "                                DD_test = operator_seq[5](operator_seq[4](operator_seq[3](operator_seq[2](operator_seq[1](operator_seq[0](D_test_x1, D_test_x3),D_test_x4), D_test_x5), D_test_x6), D_test_x7), K_test)\n",
    "\n",
    "                                sws = compute_sample_weights(y_train, lim=lim, mul=mul)\n",
    "\n",
    "                                clf = svm.SVR(kernel='precomputed', epsilon=epsilon, C=c)\n",
    "                                cvs = cross_val_score(clf, DD_train, y=y_train, cv=5, scoring='neg_mean_absolute_error', fit_params={'sample_weight':sws})\n",
    "                                clf.fit(DD_train,y_train, sample_weight=sws)\n",
    "                                prediction = clf.predict(DD_test)\n",
    "                                score = np.mean(np.abs(prediction - np.array(y_test)))\n",
    "                                results.append({'score':score, 'cvs': cvs, 'mean_cvs':cvs.mean(), 'predicted_0':prediction[0], 'predicted_1':prediction[1], 'actual_0':y_test[0], 'actual_1':y_test[1], 'direction':direction, 'bandwidth':bandwidth, 'lim':lim, 'mul':mul, 'gamma':gamma, 'epsilon':epsilon, 'c':c, 'operator_seq':operator_seq})\n",
    "\n",
    "                                print('Score: {}, Direction: {}, Bandwidth: {}, Lim: {}, Mul: {}, Gamma: {}, Epsilon: {}, C: {}'.format(score, direction, bandwidth, lim, mul, gamma, epsilon, c))\n",
    "                                print(operator_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_df_all = pd.DataFrame(results)\n",
    "res_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_all[res_df_all['mean_cvs'] == res_df_all['mean_cvs'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_df_all[(res_df_all['predicted_0'] > 5.4) & (res_df_all['predicted_1'] < 3.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_all[(res_df_all['score'] == res_df_all['score'].min())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_all.to_csv('../data/results/both/x1x3x4x5x5bx6x7K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_x1 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x1')\n",
    "test_set_x4 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x4')\n",
    "test_set_x5 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x5')\n",
    "\n",
    "D_x1 = test_set_x1[0]\n",
    "D_x4 = test_set_x4[0]\n",
    "D_x5 = test_set_x5[0]\n",
    "y = test_set_x1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = master\n",
    "K = rbf_kernel(K_train, gamma=0.0001)\n",
    "\n",
    "D_x3 = create_x3_matrix(L)\n",
    "D_x3 = laplacian_kernel(D_x3, gamma=0.0001)\n",
    "\n",
    "D_x6 = create_x6_matrix(L, max_dims=(50, 6))\n",
    "D_x6 = laplacian_kernel(D_x6, gamma=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_2 = []\n",
    "for i in range(D_x1.shape[0]):\n",
    "    y_train = np.delete(y, i, 0)\n",
    "    y_test = y[i]\n",
    "    K_train, K_test = remove_for_loo(K,i)\n",
    "    D_train_x1, D_test_x1 = remove_for_loo(D_x1,i)\n",
    "    D_train_x3, D_test_x3 = remove_for_loo(D_x3,i)\n",
    "    D_train_x4, D_test_x4 = remove_for_loo(D_x4,i)\n",
    "    D_train_x5, D_test_x5 = remove_for_loo(D_x5,i)\n",
    "    D_train_x6, D_test_x6 = remove_for_loo(D_x6,i)\n",
    "    \n",
    "    DD_train = D_train_x1+D_train_x3+D_train_x4+D_train_x5+D_train_x6+K_train\n",
    "    DD_test = D_test_x1+D_test_x3+D_test_x4+D_test_x5+D_test_x6+K_test\n",
    "    \n",
    "#     sws = compute_sample_weights(y_train, lim=2, mul=10)\n",
    "    \n",
    "#     DD_train = D_train\n",
    "#     DD_test = D_test\n",
    "#     clf = KernelRidge(kernel='precomputed', alpha=.000001)\n",
    "#     clf = KernelRidge(kernel='linear', alpha=1e-5)\n",
    "\n",
    "#     clf.fit(D_train,y_train)\n",
    "    clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "#     clf = svm.LinearSVR(C=1, epsilon=0.1)\n",
    "#     clf.fit(DD_train,y_train, sample_weight=sws)\n",
    "    clf.fit(DD_train,y_train)\n",
    "    prediction = clf.predict(DD_test)\n",
    "    results_2.append({'predicted':prediction[0], 'actual':y_test, 'key':keys[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results_2)\n",
    "print(np.median(np.abs(res_df['predicted'] - res_df['actual'])))\n",
    "res_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df = res_df[(res_df['predicted'] < 16) & (res_df['predicted'] > -5)]\n",
    "colors = ['cyan' if row['key'] == 217 else 'blue' if (row['actual'] < 2) else 'orange' if row['key'] == 216 else 'silver' for idx, row in res_df.iterrows()]\n",
    "plt.scatter(res_df['actual'], res_df['predicted'], color=colors)\n",
    "plt.plot([res_df['predicted'].min(),res_df['predicted'].max()],[res_df['predicted'].min(),res_df['predicted'].max()], 'r--')\n",
    "plt.plot(np.unique(res_df['actual']), np.poly1d(np.polyfit(res_df['actual'], res_df['predicted'], 1))(np.unique(res_df['actual'])), 'g--')\n",
    "plt.xlabel('Actual logK')\n",
    "plt.ylabel('Predicted logK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x1 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x1')\n",
    "train_set_x4 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x4')\n",
    "train_set_x5 = create_test_diagram_kernels(num_directions=1, bandwidth=2., h=1, x_type='x5')\n",
    "X1 = train_set_x1[0]\n",
    "X4 = train_set_x4[0]\n",
    "X5 = train_set_x5[0]\n",
    "\n",
    "D_test_x1 = np.delete(X1[191:193], [191,192], axis=1)\n",
    "D_test_x4 = np.delete(X4[191:193], [191,192], axis=1)\n",
    "D_test_x5 = np.delete(X5[191:193], [191,192], axis=1)\n",
    "\n",
    "D_train_x1 = np.delete(np.delete(X1, [191,192], axis=0), [191,192], axis=1)\n",
    "D_train_x4 = np.delete(np.delete(X4, [191,192], axis=0), [191,192], axis=1)\n",
    "D_train_x5 = np.delete(np.delete(X5, [191,192], axis=0), [191,192], axis=1)\n",
    "\n",
    "y = train_set_x1[1]\n",
    "y_test = y[191:193]\n",
    "y_train = np.delete(y, [191,192], axis=0)\n",
    "        \n",
    "        \n",
    "K = master\n",
    "K = rbf_kernel(K, gamma=0.0001)\n",
    "K_test = np.delete(K[191:193], [191,192], axis=1)\n",
    "K_train = np.delete(np.delete(K, [191,192], axis=0), [191,192], axis=1)\n",
    "\n",
    "X3 = create_x3_matrix(L)\n",
    "X3 = laplacian_kernel(X3, gamma=0.0001)\n",
    "D_test_x3 = np.delete(X3[191:193], [191,192], axis=1)\n",
    "D_train_x3 = np.delete(np.delete(X3, [191,192], axis=0), [191,192], axis=1)\n",
    "\n",
    "X5b = create_x5_matrix(L)[:,0].reshape(-1,1)\n",
    "X5b = rbf_kernel(X5b, gamma=0.0001)\n",
    "D_test_x5b = np.delete(X5b[191:193], [191,192], axis=1)\n",
    "D_train_x5b = np.delete(np.delete(X5b, [191,192], axis=0), [191,192], axis=1)\n",
    "\n",
    "X6 = create_x6_matrix(L, max_dims=(50, 6))\n",
    "X6 = laplacian_kernel(X6, gamma=0.0001)\n",
    "D_test_x6 = np.delete(X6[191:193], [191,192], axis=1)\n",
    "D_train_x6 = np.delete(np.delete(X6, [191,192], axis=0), [191,192], axis=1)\n",
    "\n",
    "X7 = create_x7_matrix(L)\n",
    "X7 = rbf_kernel(X7, gamma=0.0001)\n",
    "D_test_x7 = np.delete(X7[191:193], [191,192], axis=1)\n",
    "D_train_x7 = np.delete(np.delete(X7, [191,192], axis=0), [191,192], axis=1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alls = [X1, X3, X4, X5, X5b, X6, X7]\n",
    "trains = [D_train_x1,D_train_x3,D_train_x4,D_train_x5,D_train_x5b,D_train_x6,D_train_x7,K_train]\n",
    "tests = [D_test_x1,D_test_x3,D_test_x4,D_test_x5,D_test_x5b,D_test_x6,D_test_x7,K_test]\n",
    "x_names = ['x1', 'x3', 'x4', 'x5', 'x5b', 'x6', 'x7', 'X10-X17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_train = 0\n",
    "DD_test = 0\n",
    "results = []\n",
    "for i in range(len(trains)):\n",
    "    DD_train += trains[i]\n",
    "    DD_test += tests[i]\n",
    "\n",
    "    clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "    cvs = cross_val_score(clf, DD_train, y=y_train, cv=5, scoring='neg_mean_absolute_error', fit_params={'sample_weight':sws})\n",
    "    clf.fit(DD_train,y_train)\n",
    "    prediction = clf.predict(DD_test)\n",
    "    score = np.mean(np.abs(prediction - np.array(y_test)))\n",
    "    results.append({'score':score, 'cvs': cvs, 'mean_cvs':cvs.mean(), 'predicted_0':prediction[0], 'predicted_1':prediction[1], 'actual_0':y_test[0], 'actual_1':y_test[1], 'x_name': x_names[i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = [X1, X3, X4, X5, X5b, X6, X7]\n",
    "trains = [D_train_x1,D_train_x3,D_train_x4,D_train_x5,D_train_x5b,D_train_x6,D_train_x7]\n",
    "tests = [D_test_x1,D_test_x3,D_test_x4,D_test_x5,D_test_x5b,D_test_x6,D_test_x7]\n",
    "x_names = ['x1', 'x3', 'x4', 'x5', 'x5b', 'x6', 'x7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_train = K_train\n",
    "DD_test = K_test\n",
    "DD = K\n",
    "results = []\n",
    "for t in range(len(trains)):\n",
    "    DD_train += trains[t]\n",
    "    DD_test += tests[t]\n",
    "\n",
    "    clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "    cvs = cross_val_score(clf, DD_train, y=y_train, cv=5, scoring='neg_mean_absolute_error', fit_params={'sample_weight':sws})\n",
    "    clf.fit(DD_train,y_train)\n",
    "    prediction = clf.predict(DD_test)\n",
    "    score = np.mean(np.abs(prediction - np.array(y_test)))\n",
    "    results.append({'score':score, 'cvs': cvs, 'mean_cvs':cvs.mean(), 'predicted_0':prediction[0], 'predicted_1':prediction[1], 'actual_0':y_test[0], 'actual_1':y_test[1], 'x_name': x_names[t]})\n",
    "    \n",
    "    results_2 = []\n",
    "    for i in range(X1.shape[0]):\n",
    "        y_train_here = np.delete(y, i, 0)\n",
    "        y_test_here = y[i]\n",
    "        DD += alls[t]\n",
    "        DD_train_here, DD_test_here = remove_for_loo(DD, i)\n",
    "\n",
    "        clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "\n",
    "        clf.fit(DD_train_here,y_train_here)\n",
    "        prediction = clf.predict(DD_test_here)\n",
    "        results_2.append({'predicted':prediction[0], 'actual':y_test_here, 'key':keys[i]})\n",
    "\n",
    "    graph_res = pd.DataFrame(results_2)\n",
    "    colors = ['cyan' if row['key'] == 217 else 'blue' if (row['actual'] < 2) else 'orange' if row['key'] == 216 else 'silver' for idx, row in graph_res.iterrows()]\n",
    "    plt.scatter(graph_res['actual'], graph_res['predicted'], color=colors)\n",
    "    plt.plot([graph_res['predicted'].min(),graph_res['predicted'].max()],[graph_res['predicted'].min(),graph_res['predicted'].max()], 'r--')\n",
    "    plt.plot(np.unique(graph_res['actual']), np.poly1d(np.polyfit(graph_res['actual'], graph_res['predicted'], 1))(np.unique(graph_res['actual'])), 'g--')\n",
    "    plt.xlabel('Actual logK')\n",
    "    plt.ylabel('Predicted logK')\n",
    "    plt.title('X Addition: {}'.format(x_names[t]))\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_K = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_K.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_K.to_csv('../data/results/both/K_to_X7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = [X1, X3, X4, X5, X5b, X6, X7, K]\n",
    "trains = [D_train_x1,D_train_x3,D_train_x4,D_train_x5,D_train_x5b,D_train_x6,D_train_x7,K_train]\n",
    "tests = [D_test_x1,D_test_x3,D_test_x4,D_test_x5,D_test_x5b,D_test_x6,D_test_x7,K_test]\n",
    "x_names = ['x1', 'x3', 'x4', 'x5', 'x5b', 'x6', 'x7', 'X10-X17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_train_first = sum(trains)\n",
    "DD_test_first = sum(tests)\n",
    "DD = sum(alls)\n",
    "results = []\n",
    "for t in range(len(trains)):\n",
    "    DD_train = DD_train_first - trains[t]\n",
    "    DD_test += tests[t]\n",
    "\n",
    "    clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "    cvs = cross_val_score(clf, DD_train, y=y_train, cv=5, scoring='neg_mean_absolute_error', fit_params={'sample_weight':sws})\n",
    "    clf.fit(DD_train,y_train)\n",
    "    prediction = clf.predict(DD_test)\n",
    "    score = np.mean(np.abs(prediction - np.array(y_test)))\n",
    "    results.append({'score':score, 'cvs': cvs, 'mean_cvs':cvs.mean(), 'predicted_0':prediction[0], 'predicted_1':prediction[1], 'actual_0':y_test[0], 'actual_1':y_test[1], 'x_name': x_names[t]})\n",
    "    \n",
    "    results_2 = []\n",
    "    for i in range(X1.shape[0]):\n",
    "        y_train_here = np.delete(y, i, 0)\n",
    "        y_test_here = y[i]\n",
    "        DD += alls[t]\n",
    "        DD_train_here, DD_test_here = remove_for_loo(DD, i)\n",
    "\n",
    "        clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "\n",
    "        clf.fit(DD_train_here,y_train_here)\n",
    "        prediction = clf.predict(DD_test_here)\n",
    "        results_2.append({'predicted':prediction[0], 'actual':y_test_here, 'key':keys[i]})\n",
    "\n",
    "    graph_res = pd.DataFrame(results_2)\n",
    "    colors = ['cyan' if row['key'] == 217 else 'blue' if (row['actual'] < 2) else 'orange' if row['key'] == 216 else 'silver' for idx, row in graph_res.iterrows()]\n",
    "    plt.scatter(graph_res['actual'], graph_res['predicted'], color=colors)\n",
    "    plt.plot([graph_res['predicted'].min(),graph_res['predicted'].max()],[graph_res['predicted'].min(),graph_res['predicted'].max()], 'r--')\n",
    "    plt.plot(np.unique(graph_res['actual']), np.poly1d(np.polyfit(graph_res['actual'], graph_res['predicted'], 1))(np.unique(graph_res['actual'])), 'g--')\n",
    "    plt.xlabel('Actual logK')\n",
    "    plt.ylabel('Predicted logK')\n",
    "    plt.title('X Addition: {}'.format(x_names[t]))\n",
    "    plt.show()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = [X1, X3, X4, X5, X5b, X6, X7]\n",
    "trains = [D_train_x1,D_train_x3,D_train_x4,D_train_x5,D_train_x5b,D_train_x6,D_train_x7,K_train]\n",
    "tests = [D_test_x1,D_test_x3,D_test_x4,D_test_x5,D_test_x5b,D_test_x6,D_test_x7,K_test]\n",
    "x_names = ['x1', 'x3', 'x4', 'x5', 'x5b', 'x6', 'x7', 'X10-X17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for t in range(2, len(trains)+1):\n",
    "    its_train = list(itertools.combinations(trains, t))\n",
    "    its_test = list(itertools.combinations(tests, t))\n",
    "    its_alls = list(itertools.combinations(alls, t))\n",
    "    its_names = list(itertools.combinations(x_names, t))\n",
    "    for j in range(len(its_train)):\n",
    "        DD_train = sum(its_train[j])\n",
    "        DD_test = sum(its_test[j])\n",
    "        x_names_here = ';'.join(its_names[j])\n",
    "\n",
    "        clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "        cvs = cross_val_score(clf, DD_train, y=y_train, cv=5, scoring='neg_mean_absolute_error', fit_params={'sample_weight':sws})\n",
    "        clf.fit(DD_train,y_train)\n",
    "        prediction = clf.predict(DD_test)\n",
    "        score = np.mean(np.abs(prediction - np.array(y_test)))\n",
    "        result = {'score':score, 'cvs': cvs, 'mean_cvs':cvs.mean(), 'predicted_0':prediction[0], 'predicted_1':prediction[1], 'actual_0':y_test[0], 'actual_1':y_test[1], 'x_name': x_names[t]}\n",
    "\n",
    "        results_2 = []\n",
    "        DD = sum(its_alls[j])\n",
    "        for i in range(X1.shape[0]):\n",
    "            y_train_here = np.delete(y, i, 0)\n",
    "            y_test_here = y[i]\n",
    "            DD_train_here, DD_test_here = remove_for_loo(DD, i)\n",
    "\n",
    "            clf = svm.SVR(kernel='precomputed', epsilon=0.0001, C=10)\n",
    "\n",
    "            clf.fit(DD_train_here,y_train_here)\n",
    "            prediction = clf.predict(DD_test_here)\n",
    "            results_2.append({'predicted':prediction[0], 'actual':y_test_here, 'key':keys[i]})\n",
    "        \n",
    "        result['loo_score'] = \n",
    "        results.append(result)\n",
    "        graph_res = pd.DataFrame(results_2)\n",
    "        colors = ['cyan' if row['key'] == 217 else 'blue' if (row['actual'] < 2) else 'orange' if row['key'] == 216 else 'silver' for idx, row in graph_res.iterrows()]\n",
    "        plt.scatter(graph_res['actual'], graph_res['predicted'], color=colors)\n",
    "        plt.plot([graph_res['predicted'].min(),graph_res['predicted'].max()],[graph_res['predicted'].min(),graph_res['predicted'].max()], 'r--')\n",
    "        plt.plot(np.unique(graph_res['actual']), np.poly1d(np.polyfit(graph_res['actual'], graph_res['predicted'], 1))(np.unique(graph_res['actual'])), 'g--')\n",
    "        plt.xlabel('Actual logK')\n",
    "        plt.ylabel('Predicted logK')\n",
    "        plt.title('X Addition: {}'.format(x_names))\n",
    "        plt.show()\n",
    "        plt.cla()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
